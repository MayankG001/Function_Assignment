{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3eb39ec-238f-4917-bde9-aea08fdd1587",
   "metadata": {},
   "source": [
    "**Q1: What is Gradient Boosting Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e7383-f003-47b7-8467-c0d4280c1d53",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It builds an ensemble of weak prediction models, typically decision trees, in a sequential manner. Each model tries to correct the errors made by the previous models. The term \"boosting\" refers to the method of converting weak learners into strong learners by iteratively learning from the residuals of previous models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f19034-4e29-4aa5-9433-e90d98244068",
   "metadata": {},
   "source": [
    "**Q2: Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea12a0d-99d8-4a67-9dc7-1de3ac47b5df",
   "metadata": {},
   "source": [
    "To implement a simple gradient boosting algorithm from scratch using Python and NumPy, we will follow these steps:\n",
    "\n",
    "* Generate a simple dataset.\n",
    "* Initialize the model with a simple prediction (e.g., mean of target values).\n",
    "* Iteratively train weak learners on the residuals.\n",
    "* Combine the weak learners to form the final prediction.\n",
    "* Evaluate the model using mean squared error (MSE) and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c12e780-8f98-48c3-8db7-092d260e00fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.8196394762795383\n",
      "R-squared: 0.5696709398782804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 4 * X.squeeze() + np.random.randn(100)  # Simple linear relation with noise\n",
    "\n",
    "# Number of boosting rounds\n",
    "n_estimators = 100\n",
    "# Learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize predictions\n",
    "predictions = np.full(y.shape, np.mean(y))\n",
    "\n",
    "# Store predictions at each iteration\n",
    "all_predictions = [predictions.copy()]\n",
    "\n",
    "for _ in range(n_estimators):\n",
    "    # Compute the residuals\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Fit a simple decision stump (tree with 1 split) on residuals\n",
    "    split_idx = np.argmin(residuals)\n",
    "    split_value = X[split_idx, 0]\n",
    "    left_mask = X[:, 0] <= split_value\n",
    "    right_mask = ~left_mask\n",
    "    \n",
    "    # Simple model predictions\n",
    "    left_value = np.mean(residuals[left_mask])\n",
    "    right_value = np.mean(residuals[right_mask])\n",
    "    \n",
    "    # Update predictions\n",
    "    predictions[left_mask] += learning_rate * left_value\n",
    "    predictions[right_mask] += learning_rate * right_value\n",
    "    \n",
    "    all_predictions.append(predictions.copy())\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ab7e3-7100-4e0f-8559-950ae433145a",
   "metadata": {},
   "source": [
    "**Q3: Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a6145e-f752-451e-a57a-32aea0911211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "Best score: -0.8776423835049223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a simple dataset\n",
    "X = np.random.rand(100, 1)\n",
    "y = 4 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Define the model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')\n",
    "print(f'Best score: {grid_search.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929a34d-8c4a-4f89-9696-c4d20219dad6",
   "metadata": {},
   "source": [
    "**Q4: What is a weak learner in Gradient Boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd3499-6b3b-4600-9aa1-f77e40a3aba1",
   "metadata": {},
   "source": [
    "A weak learner in Gradient Boosting is a model that performs slightly better than random guessing. In the context of Gradient Boosting, it is usually a simple decision tree with limited depth. The idea is to combine many such weak learners to build a strong predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f051c59-e9e7-4f81-9181-602e33c7a937",
   "metadata": {},
   "source": [
    "**Q5: What is the intuition behind the Gradient Boosting algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516bce8e-f7bf-4178-aa1d-7a280a004e87",
   "metadata": {},
   "source": [
    "The intuition behind Gradient Boosting is to iteratively reduce the prediction errors by adding new models that correct the errors of the existing ensemble. Each new model is trained to predict the residuals of the current model, effectively \"boosting\" its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695697e4-66aa-4bc6-8dcf-86d5a7e49fac",
   "metadata": {},
   "source": [
    "**Q6: How does Gradient Boosting algorithm build an ensemble of weak learners?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306af3a9-c170-4284-a861-7543d62b3413",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an ensemble of weak learners by sequentially adding new models that predict the residuals of the current ensemble. Each new model is trained on the errors made by the previous models, and the predictions are combined using a weighted sum to make the final prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3bb27d-2bff-41af-bcc4-dd891aecb423",
   "metadata": {},
   "source": [
    "**Q7: What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7188af-b6a1-4321-815f-ec322eb77574",
   "metadata": {},
   "source": [
    "The steps involved in constructing the mathematical intuition of Gradient Boosting are:\n",
    "\n",
    "1. Initialize the model with a constant prediction (e.g., the mean of the target values).\n",
    "2. Iteratively add new models:\n",
    "   * Compute the residuals (errors) of the current model.\n",
    "   * Fit a new weak learner to the residuals.\n",
    "   * Update the ensemble by adding the new modelâ€™s predictions, scaled by a learning rate.\n",
    "3. Repeat the process for a predefined number of iterations or until the residuals are minimized.\n",
    "4. Combine all weak learners to form the final prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
