{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0e5879-2b06-4914-8dbb-cdc650ab1f92",
   "metadata": {},
   "source": [
    "Q1: What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0712b-578c-4f95-90ec-65e219196e91",
   "metadata": {},
   "source": [
    "* Min-Max scaling is a normalization technique that scales the features of a dataset to a fixed range, typically [0, 1] or [-1, 1].\n",
    "* It is used in data preprocessing to ensure that features contribute equally to the model's performance and to avoid biases due to differing feature scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c38d0c1-c27d-4147-8b86-95899d506b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1], [5], [10], [15], [20]])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adee916-87f7-431c-a535-f2ebb8639f2a",
   "metadata": {},
   "source": [
    "Q2: What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498a0a7-001a-43b6-8b6f-29ec07fef599",
   "metadata": {},
   "source": [
    "* Unit Vector scaling scales each feature vector to have a unit norm (length of 1). This technique is useful when the direction of the data is more important than its magnitude.\n",
    "* It differs from Min-Max scaling, which scales the data to a specific range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a601a38-2522-4de7-9577-fb52f476e7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26726124 0.53452248 0.80178373]\n",
      " [0.45584231 0.56980288 0.68376346]\n",
      " [0.50257071 0.57436653 0.64616234]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "normalized_data = normalize(data, norm='l2')\n",
    "print(normalized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941a761-f4db-4036-b45d-a6fbb771a342",
   "metadata": {},
   "source": [
    "Q3: What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c3853-2271-4487-88d3-b26ffc790cc4",
   "metadata": {},
   "source": [
    "* PCA (Principal Component Analysis) is a statistical technique used to reduce the dimensionality of a dataset by transforming the data into a new set of orthogonal axes (principal components) that capture the maximum variance.\n",
    "* It helps in reducing the number of features while retaining most of the original variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4a7b82-350e-40cc-99f3-d71fe3367143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44362444]\n",
      " [ 2.17719404]\n",
      " [-0.57071239]\n",
      " [ 0.12902465]\n",
      " [-1.29188186]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]])\n",
    "pca = PCA(n_components=1)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabf85f-e4a0-4bc9-b57c-22c6f8c85822",
   "metadata": {},
   "source": [
    "Q4: What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fa2b0-7704-49e6-ba8f-e6ccd55ca012",
   "metadata": {},
   "source": [
    "* Relationship: PCA is a technique for Feature Extraction that transforms the original features into a new set of features (principal components) that capture the most variance in the data.\n",
    "* Usage: PCA can be used for Feature Extraction by selecting the top principal components as the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5729a0-468c-4351-a3f3-5271e6b9f70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44362444 -0.20099093]\n",
      " [ 2.17719404 -0.05500992]\n",
      " [-0.57071239  0.36808609]\n",
      " [ 0.12902465  0.06747325]\n",
      " [-1.29188186 -0.17955849]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]])\n",
    "pca = PCA(n_components=2)\n",
    "pca_features = pca.fit_transform(data)\n",
    "print(pca_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95fd37-8e77-4d8b-a805-a70a2f148c0d",
   "metadata": {},
   "source": [
    "Q5: You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f1c62-86a0-4174-8eca-90e9ae4e437b",
   "metadata": {},
   "source": [
    "**Steps to use Min-Max scaling:**\n",
    "1. Import the necessary library: from sklearn.preprocessing import MinMaxScaler.\n",
    "2. Load the data: Assume features are price, rating, and delivery_time.\n",
    "3. Initialize the scaler: scaler = MinMaxScaler().\n",
    "4. Fit and transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92603e7-8e37-4a56-a411-4ac820e2b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[price, rating, delivery_time]])\n",
    "scaled_data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d90b4-4dcc-4bb4-b3ab-cb232831753c",
   "metadata": {},
   "source": [
    "Q6: You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080788c-45ed-4b7d-8a59-191c27f4da28",
   "metadata": {},
   "source": [
    "**Steps to use PCA:**\n",
    "1. Import the necessary library: from sklearn.decomposition import PCA.\n",
    "2. Load the data: Assume data contains financial and market trend features.\n",
    "3. Standardize the data: PCA works better with standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ee5f096-4e68-4c7e-b733-691fa58c1dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4bf53-06de-4830-b60e-3584e7dcf101",
   "metadata": {},
   "source": [
    "4. Initialize PCA: Decide on the number of components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa2692-d31e-4364-a7ac-264e6d4daf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=desired_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c605999-0d0b-4454-887e-71c22e9169b5",
   "metadata": {},
   "source": [
    "5. Fit and transform the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9383e305-0fef-4571-862c-c92bb3f445f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = pca.fit_transform(data_standardized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b1394-6c26-4c87-b72d-94ac856ec12e",
   "metadata": {},
   "source": [
    "Q7: For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8edcef-0f68-4624-80fd-45ab6adca421",
   "metadata": {},
   "source": [
    "**Steps to perform Min-Max scaling:**\n",
    "1. Import the necessary library: from sklearn.preprocessing import MinMaxScaler.\n",
    "2. Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa80b0fd-99f0-427b-a188-0ee90a178d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[1], [5], [10], [15], [20]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2b83af-71ac-422f-98d9-6d8ff2a1a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Initialize the scaler with range -1 to 1:\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "561170f4-b346-4de8-904f-5bd5e0a60eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 4.Fit and transform the data:\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4c015-b5f9-4d89-853a-408f133a9b90",
   "metadata": {},
   "source": [
    "Q8: For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156ae75-0cb1-415b-bdc3-97c264baffb5",
   "metadata": {},
   "source": [
    "**Steps to use PCA:**\n",
    "\n",
    "1. Import the necessary library: from sklearn.decomposition import PCA.\n",
    "2. Load the data: Assume data contains the features.\n",
    "3. Standardize the data: PCA works better with standardized data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddcab45e-00c6-4bff-8cd2-9a67008036e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746039f4-b110-412a-9f05-3e6946756477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Initialize PCA: Choose the number of components.\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(data_standardized)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2f5f4-a0f0-43b0-b03c-17f48b018e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Choose the number of components: Retain the number of components that explain a significant portion of the variance (e.g., 95%).\n",
    "num_components = np.where(cumulative_variance >= 0.95)[0][0] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07572dc3-5409-4a81-a5c2-1b6c1fc07a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Fit and transform the data.\n",
    "pca = PCA(n_components=num_components)\n",
    "reduced_data = pca.fit_transform(data_standardized)\n",
    "print(reduced_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
