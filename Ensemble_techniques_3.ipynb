{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5516e18-2ca3-45e6-897a-77687c76fc3a",
   "metadata": {},
   "source": [
    "**Q1. You are work#ng on a machine learn#ng project where you have a dataset containing numerical and\n",
    "categorical features. You have #dent#f#ed that some of the features are highly correlated and there are\n",
    "missing values #n some of the columns. You want to bu#ld a p#pel#ne that automates the feature\n",
    "eng#neer#ng process and handles the m#ss#ng values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e8462-175e-47aa-bc59-5f0ba29bfd10",
   "metadata": {},
   "source": [
    "1. Automated feature selection:\n",
    "We can use SelectKBest with f_classif as the scoring function for feature selection.\n",
    "\n",
    "2. Numerical pipeline:\n",
    "   * Impute missing values with the mean.\n",
    "   * Scale the features using standardization.\n",
    "\n",
    "3. Categorical pipeline:\n",
    "   * Impute missing values with the most frequent value.\n",
    "   * One-hot encode the categorical features.\n",
    "\n",
    "4. Combine pipelines:\n",
    "Use ColumnTransformer to combine numerical and categorical pipelines.\n",
    "\n",
    "5. Model training and evaluation:\n",
    "Use a RandomForestClassifier and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52afaf3f-7616-441a-baf5-107e7c2715a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual dataset)\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# For illustration, using a sample dataset from scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris(as_frame=True)\n",
    "df = data.frame\n",
    "df['target'] = data.target\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Numerical pipeline\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_cols),\n",
    "        ('cat', categorical_pipeline, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Feature selection\n",
    "feature_selection = SelectKBest(score_func=f_classif, k='all')  # Adjust 'k' as needed\n",
    "\n",
    "# Complete pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e447ca-13f3-4f28-ad33-e243b90eb9e7",
   "metadata": {},
   "source": [
    "**Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
    "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
    "accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa849dfd-38d0-4eeb-8ba5-4388708ca846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier model accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create the voting classifier with RandomForest and LogisticRegression\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
    "    ],\n",
    "    voting='hard'  # Use 'soft' if you want to use predicted probabilities\n",
    ")\n",
    "\n",
    "# Complete pipeline with voting classifier\n",
    "voting_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', voting_clf)\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "voting_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_voting = voting_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(f'Voting classifier model accuracy: {accuracy_voting:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de154827-43b7-4158-a535-62dbd942062c",
   "metadata": {},
   "source": [
    "`Interpretation and Suggestions`\n",
    "1. Accuracy Interpretation:\n",
    "   * The accuracy scores from both models (random forest alone and the voting classifier) give an idea of how well the models are performing.\n",
    "   * Compare the accuracy of the voting classifier with the individual random forest model to determine if the ensemble method improves performance.\n",
    "\n",
    "2. Possible Improvements:\n",
    "   * Feature Engineering: Explore additional feature engineering techniques to improve model performance.\n",
    "   * Hyperparameter Tuning: Use grid search or randomized search to tune the hyperparameters of the classifiers.\n",
    "   * Cross-Validation: Employ cross-validation to get a more robust estimate of the model performance.\n",
    "   * Handling Class Imbalance: If the dataset is imbalanced, consider techniques like SMOTE or class weighting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
