{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a15726-f218-4780-a9da-b1ff0b8e1802",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7168cd-7777-4347-804c-f1e53c560227",
   "metadata": {},
   "source": [
    "`Web Scraping:`\n",
    "Web scraping is the automated process of extracting data from websites. It involves fetching the web page content and parsing it to retrieve specific information.\n",
    "\n",
    "`Why is it Used?:`\n",
    "\n",
    "* To gather large amounts of data quickly and efficiently.\n",
    "* To automate data collection for analysis, research, or business intelligence.\n",
    "* To monitor changes on websites, such as price tracking or news updates.\n",
    "\n",
    "`Three Areas Where Web Scraping is Used:`\n",
    "\n",
    "1. E-commerce: To track product prices, availability, and reviews across different platforms.\n",
    "2. Market Research: To collect data on competitors, customer sentiment, and industry trends.\n",
    "3. Academic Research: To gather data for studies, surveys, or analysis from various online sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e406a1e-8542-4a13-ac89-297ab45fc843",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842124c-3254-4cef-8a8a-4f29b006d3c5",
   "metadata": {},
   "source": [
    "1. Manual Scraping: Copying and pasting data from web pages manually. This method is time-consuming and not scalable.\n",
    "\n",
    "2. HTML Parsing: Using libraries like Beautiful Soup or lxml in Python to parse HTML content and extract data.\n",
    "\n",
    "3. Browser Automation: Using tools like Selenium to automate browser actions, allowing for interaction with dynamic content (e.g., JavaScript-rendered pages).\n",
    "\n",
    "4. APIs: Some websites provide APIs that allow for structured data access without scraping. This is often the preferred method when available.\n",
    "\n",
    "5. Web Scraping Frameworks: Using frameworks like Scrapy, which provide built-in tools for scraping, data processing, and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f65746-b706-45e5-88bb-a57f08df7860",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435787d-b416-4434-8e3e-d97ddd239c68",
   "metadata": {},
   "source": [
    "`Beautiful Soup:`\n",
    "Beautiful Soup is a Python library used for parsing HTML and XML documents. It creates a parse tree for parsing HTML and allows for easy navigation and searching of the document.\n",
    "\n",
    "`Why is it Used?:`\n",
    "\n",
    "* To simplify the process of extracting data from web pages.\n",
    "* To handle poorly formatted HTML and XML documents.\n",
    "* To provide a Pythonic way to navigate and search the parse tree, making it easier to extract specific elements or attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5af355-9807-4e31-94d8-c00a4ec70ee2",
   "metadata": {},
   "source": [
    "Q4. Why is Flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45613f42-eed4-47b9-811e-ecac522bf0a7",
   "metadata": {},
   "source": [
    "`Flask:`\n",
    "Flask is a lightweight web framework for Python that is used to build web applications.\n",
    "\n",
    "`Why is it Used in Web Scraping Projects?:`\n",
    "\n",
    "* To create a web interface for users to interact with the scraping functionality.\n",
    "* To serve the scraped data through APIs, allowing for easy access and integration with other applications.\n",
    "* To manage the scraping process, including scheduling and monitoring, in a user-friendly manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db0148-cbb3-40ac-8789-e4b56a28acf3",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a8e13-d35e-421f-b3e3-b742abc9d557",
   "metadata": {},
   "source": [
    "1. `Amazon EC2 (Elastic Compute Cloud):`\n",
    "\n",
    "Used to host the web scraping application. It provides scalable computing capacity in the cloud, allowing you to run your Flask application.\n",
    "\n",
    "2. `Amazon S3 (Simple Storage Service):`\n",
    "\n",
    "Used to store the scraped data. S3 provides durable and scalable storage for data, making it easy to retrieve and manage.\n",
    "\n",
    "3. `AWS Lambda:`\n",
    "\n",
    "Can be used to run the scraping scripts in a serverless environment. This allows for automatic execution of scraping tasks without managing servers.\n",
    "\n",
    "4. `Amazon RDS (Relational Database Service):`\n",
    "\n",
    "Used to store structured data in a relational database. RDS makes it easy to set up, operate, and scale a relational database in the cloud.\n",
    "\n",
    "5. `Amazon CloudWatch:`\n",
    "\n",
    "Used for monitoring and logging the performance of the web scraping application. It helps in tracking metrics and setting alarms for various operational parameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
